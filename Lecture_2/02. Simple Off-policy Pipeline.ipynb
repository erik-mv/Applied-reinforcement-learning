{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVM3b5XNVU-H"
   },
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DyxgIFNwbKhO"
   },
   "outputs": [],
   "source": [
    "class ExpirienceReplay:\n",
    "    def __init__(self, size=10000):\n",
    "        self.data = deque(maxlen=size)\n",
    "    \n",
    "    def add(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def sample(self, size):\n",
    "        batch = random.sample(self.data, size)\n",
    "        return list(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdAb5L1ZVSIV"
   },
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KmDASjl-emU9"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, action_size):\n",
    "        pass \n",
    "\n",
    "    def act(self, state):\n",
    "        pass\n",
    "\n",
    "\n",
    "class EpsilonGreedyAgent(Agent):\n",
    "    def __init__(self, action_size, agent, eps=0.1):\n",
    "        super().__init__(action_size)\n",
    "        self.agent = agent\n",
    "        self.eps = eps\n",
    "        self.action_size = action_size\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() < self.eps:\n",
    "            return random.randint(self.action_size)\n",
    "        else:\n",
    "            return self.agent.act(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w97HDpRaVYPt"
   },
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "go_97sTEfY25"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, agent, experience_replay_size=1000000, batch_size=64):\n",
    "        self.agent = agent\n",
    "        self.buffer = ExpirienceReplay(experience_replay_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def consume_transitions(self, transitions):\n",
    "        for t in transitions:\n",
    "            self.buffer.add(t)\n",
    "    \n",
    "    def update(self):\n",
    "        batch = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        state, action, next_state, reward, done = batch\n",
    "        state = torch.tensor(np.array(state, dtype=np.float32))\n",
    "        action = torch.tensor(np.array(action, dtype=np.int))\n",
    "        next_state = torch.tensor(np.array(next_state, dtype=np.float32))\n",
    "        reward = torch.tensor(np.array(reward, dtype=np.float32))\n",
    "        done = torch.tensor(np.array(done, dtype=np.float32))\n",
    "\n",
    "        metrics = self._update_agent((state, action, next_state, reward, done))\n",
    "        return metrics\n",
    "\n",
    "\n",
    "    def _update_agent(self, batch):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9IQdTREVch7"
   },
   "source": [
    "## Environment Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HlJSAGnHgX7j"
   },
   "outputs": [],
   "source": [
    "class EnvWorker:\n",
    "    def __init__(self, env_name, agent):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.agent = agent\n",
    "        self.current_state = self.env.reset()\n",
    "\n",
    "    def collect_transitions(self, num_transitions):\n",
    "        transititons = []\n",
    "        for _ in range(num_transitions):\n",
    "            action = self.agent.act(self.current_state)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            transitions.append([\n",
    "                    self.current_state, \n",
    "                    action, \n",
    "                    next_state, \n",
    "                    reward, \n",
    "                    done\n",
    "            ])\n",
    "            self.current_state = self.env.reset() if done else next_state\n",
    "        return transitions\n",
    "        \n",
    "    def collect_trajectories(self, num_trajectories):\n",
    "        trajectories = []\n",
    "        for _ in range(num_trajectories):\n",
    "            transitions = []\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.agent.act(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                transitions.append([\n",
    "                        state, \n",
    "                        action, \n",
    "                        next_state, \n",
    "                        reward, \n",
    "                        done\n",
    "                ])\n",
    "                state = self.env.reset() if done else next_state\n",
    "            trajectories.append(transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYn-qiGyVgwA"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jYEetT7knlJS"
   },
   "outputs": [],
   "source": [
    "def train(agent: Agent, exploration_agent: Agent, trainer: Trainer, \n",
    "          env_name, experience_per_update, updates_total,\n",
    "          updates_per_evalutaion, episodes_per_evaluation):\n",
    "    \n",
    "    exploration_env_worker = EnvWorker(env_name, exploration_agent)\n",
    "    exploitation_env_worker = EnvWorker(env_name, agent)\n",
    "\n",
    "    for step in range(updates_total):\n",
    "        transitions = exploration_env_worker.collect_transitions(\n",
    "            experience_per_update)\n",
    "        trainer.consume_transitions(transitions)\n",
    "        metrics = trainer.update()\n",
    "\n",
    "        if step % updates_per_evalutaion == 0:\n",
    "            trajectories = exploitation_env_worker.collect_trajectories(\n",
    "                episodes_per_evaluation)\n",
    "            total_rewards = [sum([t[3] for t in transitions])\n",
    "                             for transitions in trajectories]\n",
    "            print(f\"Step {step} | Mean reward: {np.mean(total_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_paNkPPeuZ1F"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'action_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13760/3094433175.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Здесь будет агент конкретного алгоритма\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Опять же Trainer для конкретного алгоритма\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mexploration_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEpsilonGreedyAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexploration_agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"LunarLander-v2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'action_size'"
     ]
    }
   ],
   "source": [
    "agent = Agent() # Здесь будет агент конкретного алгоритма\n",
    "trainer = Trainer(agent, 500000, 64) # Опять же Trainer для конкретного алгоритма\n",
    "exploration_agent = EpsilonGreedyAgent(agent)\n",
    "\n",
    "train(agent, exploration_agent, trainer, \"LunarLander-v2\", 4, 100000, 1000, 10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM8QbThHrusK027x+TkMwY0",
   "name": "02. Simple Off-policy Pipeline",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
