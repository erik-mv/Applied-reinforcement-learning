{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02. Simple Off-policy Pipeline","provenance":[],"authorship_tag":"ABX9TyM8QbThHrusK027x+TkMwY0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cVM3b5XNVU-H"},"source":["## Experience Replay"]},{"cell_type":"code","metadata":{"id":"DyxgIFNwbKhO"},"source":["class ExpirienceReplay:\n","    def __init__(self, size=10000):\n","        self.data = deque(maxlen=size)\n","    \n","    def add(self, transition):\n","        self.data.append(transition)\n","        \n","    def sample(self, size):\n","        batch = random.sample(self.data, size)\n","        return list(zip(*batch))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FdAb5L1ZVSIV"},"source":["## Agent"]},{"cell_type":"code","metadata":{"id":"KmDASjl-emU9"},"source":["class Agent:\n","    def __init__(self, action_size):\n","        pass \n","\n","    def act(self, state):\n","        pass\n","\n","\n","class EpsilonGreedyAgent(Agent):\n","    def __init__(self, action_size, agent, eps=0.1):\n","        super().__init__(action_size)\n","        self.agent = agent\n","        self.eps = eps\n","        self.action_size = action_size\n","\n","    def act(self, state):\n","        if random.random() < self.eps:\n","            return random.randint(self.action_size)\n","        else:\n","            return self.agent.act(state)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w97HDpRaVYPt"},"source":["## Trainer"]},{"cell_type":"code","metadata":{"id":"go_97sTEfY25"},"source":["class Trainer:\n","    def __init__(self, agent, experience_replay_size=1000000, batch_size=64):\n","        self.agent = agent\n","        self.buffer = ExpirienceReplay(experience_replay_size)\n","        self.batch_size = batch_size\n","\n","    def consume_transitions(self, transitions):\n","        for t in transitions:\n","            self.buffer.add(t)\n","    \n","    def update(self):\n","        batch = self.buffer.sample(self.batch_size)\n","\n","        state, action, next_state, reward, done = batch\n","        state = torch.tensor(np.array(state, dtype=np.float32))\n","        action = torch.tensor(np.array(action, dtype=np.int))\n","        next_state = torch.tensor(np.array(next_state, dtype=np.float32))\n","        reward = torch.tensor(np.array(reward, dtype=np.float32))\n","        done = torch.tensor(np.array(done, dtype=np.float32))\n","\n","        metrics = self._update_agent((state, action, next_state, reward, done))\n","        return metrics\n","\n","\n","    def _update_agent(self, batch):\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D9IQdTREVch7"},"source":["## Environment Worker"]},{"cell_type":"code","metadata":{"id":"HlJSAGnHgX7j"},"source":["class EnvWorker:\n","    def __init__(self, env_name, agent):\n","        self.env = gym.make(env_name)\n","        self.agent = agent\n","        self.current_state = self.env.reset()\n","\n","    def collect_transitions(self, num_transitions):\n","        transititons = []\n","        for _ in range(num_transitions):\n","            action = self.agent.act(self.current_state)\n","            next_state, reward, done, _ = self.env.step(action)\n","            transitions.append([\n","                    self.current_state, \n","                    action, \n","                    next_state, \n","                    reward, \n","                    done\n","            ])\n","            self.current_state = self.env.reset() if done else next_state\n","        return transitions\n","        \n","    def collect_trajectories(self, num_trajectories):\n","        trajectories = []\n","        for _ in range(num_trajectories):\n","            transitions = []\n","            state = self.env.reset()\n","            done = False\n","            while not done:\n","                action = self.agent.act(state)\n","                next_state, reward, done, _ = self.env.step(action)\n","                transitions.append([\n","                        state, \n","                        action, \n","                        next_state, \n","                        reward, \n","                        done\n","                ])\n","                state = self.env.reset() if done else next_state\n","            trajectories.append(transitions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sYn-qiGyVgwA"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"jYEetT7knlJS"},"source":["def train(agent: Agent, exploration_agent: Agent, trainer: Trainer, \n","          env_name, experience_per_update, updates_total,\n","          updates_per_evalutaion, episodes_per_evaluation):\n","    \n","    exploration_env_worker = EnvWorker(env_name, exploration_agent)\n","    exploitation_env_worker = EnvWorker(env_name, agent)\n","\n","    for step in range(updates_total):\n","        transitions = exploration_env_worker.collect_transitions(\n","            experience_per_update)\n","        trainer.consume_transitions(transitions)\n","        metrics = trainer.update()\n","\n","        if step % updates_per_evalutaion == 0:\n","            trajectories = exploitation_env_worker.collect_trajectories(\n","                episodes_per_evaluation)\n","            total_rewards = [sum([t[3] for t in transitions])\n","                             for transitions in trajectories]\n","            print(f\"Step {step} | Mean reward: {np.mean(total_rewards)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_paNkPPeuZ1F"},"source":["agent = Agent() # Здесь будет агент конкретного алгоритма\n","trainer = Trainer(agent, 500000, 64) # Опять же Trainer для конкретного алгоритма\n","exploration_agent = EpsilonGreedyAgent(agent)\n","\n","train(agent, exploration_agent, trainer, \"LunarLander-v2\", 4, 100000, 1000, 10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"szD1MA_BMx_3","executionInfo":{"status":"ok","timestamp":1634137898884,"user_tz":-180,"elapsed":311,"user":{"displayName":"Oleg Svidchenko","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjtLmACOAq82avmVoshNzzAaxoCoazUityt1Bk5=s64","userId":"05803297682533666498"}}},"source":[""],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jc3szhWNM_lT"},"source":[""],"execution_count":null,"outputs":[]}]}